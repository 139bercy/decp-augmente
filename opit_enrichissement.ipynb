{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import numpy as n\n",
    "import os\n",
    "import dask.dataframe as dd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siretdf_from_original_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Utilisation d'un dataframe intermediaire pour traiter les Siret unique\n",
    "\n",
    "    Retour:\n",
    "        - pd.DataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    dfSIRET = pd.DataFrame.copy(df[['idTitulaires', 'typeIdentifiant', 'denominationSociale']])\n",
    "    dfSIRET = dfSIRET.drop_duplicates(subset=['idTitulaires'], keep='first')\n",
    "    dfSIRET.reset_index(inplace=True, drop=True)\n",
    "    dfSIRET.idTitulaires = dfSIRET.idTitulaires.astype(str)\n",
    "\n",
    "    dfSIRET[\"idTitulaires\"] = np.where(~dfSIRET[\"idTitulaires\"].str.isdigit(), '00000000000000', dfSIRET.idTitulaires)\n",
    "\n",
    "    dfSIRET.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    dfSIRET.rename(columns={\n",
    "        \"idTitulaires\": \"siret\",\n",
    "        \"typeIdentifiant\": \"siren\"}, inplace=True)\n",
    "    dfSIRET.siren = dfSIRET.siret.str[:9] # 9 = taille du Siren\n",
    "    dfSIRET.denominationSociale = dfSIRET.denominationSociale.astype(str)\n",
    "\n",
    "    return dfSIRET\n",
    "def getArchiveErrorSIRET() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Récupération des siret erronés\n",
    "\n",
    "    Retour:\n",
    "        - pd.DataFrame\n",
    "    \"\"\"\n",
    "    archiveErrorSIRET = pd.DataFrame(columns=['siret', 'siren', 'denominationSociale'])\n",
    "    return archiveErrorSIRET\n",
    "def get_enrichissement_insee(dfSIRET: pd.DataFrame, path_to_data: str) -> list:\n",
    "    \"\"\"\n",
    "    Ajout des informations Adresse/Activité des entreprises via la base siren Insee\n",
    "\n",
    "    Retour:\n",
    "        - list:\n",
    "            - list[0]: pd.DataFrame -- données principales\n",
    "            - list[1]: pd.DataFrame -- données ou le SIRET n'est pas renseigné\n",
    "    \"\"\"\n",
    "    # dans StockEtablissement_utf8, il y a principalement : siren, siret, nom établissement, adresse, activité principale\n",
    "    path = os.path.join(path_to_data, conf_data[\"base_sirene_insee\"])\n",
    "    columns = [\n",
    "        'siren',\n",
    "        'nic',\n",
    "        'siret',\n",
    "        'typeVoieEtablissement',\n",
    "        'libelleVoieEtablissement',\n",
    "        'codePostalEtablissement',\n",
    "        'libelleCommuneEtablissement',\n",
    "        'codeCommuneEtablissement',\n",
    "        'activitePrincipaleEtablissement',\n",
    "        'nomenclatureActivitePrincipaleEtablissement']  # Colonne à utiliser dans la base Siren\n",
    "    dtypes = {\n",
    "        'siret': 'string',\n",
    "        'typeVoieEtablissement': 'string',\n",
    "        'libelleVoieEtablissement': 'string',\n",
    "        'codePostalEtablissement': 'string',\n",
    "        'libelleCommuneEtablissement': 'string',\n",
    "    }\n",
    "\n",
    "    result = pd.DataFrame(columns=columns)\n",
    "    chunksize = 1000000\n",
    "    for gm_chunk in pd.read_csv(path, chunksize=chunksize, sep=',', encoding='utf-8', usecols=columns, dtype=dtypes):\n",
    "        resultTemp = pd.merge(dfSIRET['siret'], gm_chunk, on=['siret'], copy=False)\n",
    "        result = pd.concat([result, resultTemp], axis=0, copy=False)\n",
    "        del resultTemp\n",
    "    result = result.drop_duplicates(subset=['siret'], keep='first')\n",
    "\n",
    "    enrichissement_insee_siret = pd.merge(dfSIRET, result, how='outer', on=['siret'], copy=False)\n",
    "    enrichissement_insee_siret.rename(columns={\"siren_x\": \"siren\"}, inplace=True)\n",
    "    enrichissement_insee_siret.drop(columns=[\"siren_y\"], axis=1, inplace=True)\n",
    "    nanSiret = enrichissement_insee_siret[enrichissement_insee_siret.activitePrincipaleEtablissement.isnull()]\n",
    "    enrichissement_insee_siret = enrichissement_insee_siret[\n",
    "        enrichissement_insee_siret.activitePrincipaleEtablissement.notnull()]\n",
    "    nanSiret = nanSiret.loc[:, [\"siret\", \"siren\", \"denominationSociale\"]]\n",
    "\n",
    "    # Concaténation des deux resultats\n",
    "    enrichissementInsee = enrichissement_insee_siret\n",
    "\n",
    "    temp_df = pd.merge(nanSiret, result, indicator=True, how=\"outer\", on='siren', copy=False)\n",
    "    del result\n",
    "    nanSiret = temp_df[temp_df['activitePrincipaleEtablissement'].isnull()]\n",
    "    nanSiret = nanSiret.iloc[:, :3]\n",
    "    nanSiret.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return [enrichissementInsee, nanSiret]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_nettoye', 'rb') as df_nettoye:\n",
    "    df = pickle.load(df_nettoye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfSIRET = get_siretdf_from_original_data(df)\n",
    "archiveErrorSIRET = getArchiveErrorSIRET()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vestige de code avec DaskDataFrames, que j'avais reussi a faire marcher sur l'autre PC avant que ça crash. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not all divisions are known, can't align partitions. Please use `set_index` to set the index.. If you don't want the partitions to be aligned, and are calling `map_partitions` directly, pass `align_dataframes=False`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/dask/dataframe/core.py:6618\u001b[0m, in \u001b[0;36mmap_partitions\u001b[0;34m(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6617\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 6618\u001b[0m     args \u001b[39m=\u001b[39m _maybe_align_partitions(args)\n\u001b[1;32m   6619\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/dask/dataframe/multi.py:174\u001b[0m, in \u001b[0;36m_maybe_align_partitions\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(df\u001b[39m.\u001b[39mdivisions \u001b[39m==\u001b[39m divisions \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs):\n\u001b[0;32m--> 174\u001b[0m     dfs2 \u001b[39m=\u001b[39m \u001b[39miter\u001b[39m(align_partitions(\u001b[39m*\u001b[39;49mdfs)[\u001b[39m0\u001b[39m])\n\u001b[1;32m    175\u001b[0m     \u001b[39mreturn\u001b[39;00m [a \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(a, _Frame) \u001b[39melse\u001b[39;00m \u001b[39mnext\u001b[39m(dfs2) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m args]\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/dask/dataframe/multi.py:128\u001b[0m, in \u001b[0;36malign_partitions\u001b[0;34m(*dfs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mall\u001b[39m(df\u001b[39m.\u001b[39mknown_divisions \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs1):\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNot all divisions are known, can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt align \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpartitions. Please use `set_index` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    131\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mto set the index.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    132\u001b[0m     )\n\u001b[1;32m    134\u001b[0m divisions \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(unique(merge_sorted(\u001b[39m*\u001b[39m[df\u001b[39m.\u001b[39mdivisions \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs1])))\n",
      "\u001b[0;31mValueError\u001b[0m: Not all divisions are known, can't align partitions. Please use `set_index` to set the index.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ddtest \u001b[39m=\u001b[39m ddcache\u001b[39m.\u001b[39;49mmap_partitions( find_missing_siret, dfSIRET, dfcache\u001b[39m=\u001b[39;49mpd\u001b[39m.\u001b[39;49mDataFrame(data\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39msiret\u001b[39;49m\u001b[39m\"\u001b[39;49m: [\u001b[39m\"\u001b[39;49m\u001b[39mempty_cache\u001b[39;49m\u001b[39m\"\u001b[39;49m]}))\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/dask/dataframe/core.py:872\u001b[0m, in \u001b[0;36m_Frame.map_partitions\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    744\u001b[0m \u001b[39m@insert_meta_param_description\u001b[39m(pad\u001b[39m=\u001b[39m\u001b[39m12\u001b[39m)\n\u001b[1;32m    745\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmap_partitions\u001b[39m(\u001b[39mself\u001b[39m, func, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    746\u001b[0m     \u001b[39m\"\"\"Apply Python function on each DataFrame partition.\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \n\u001b[1;32m    748\u001b[0m \u001b[39m    Note that the index and divisions are assumed to remain unchanged.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39m    None as the division.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m     \u001b[39mreturn\u001b[39;00m map_partitions(func, \u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/dask/dataframe/core.py:6620\u001b[0m, in \u001b[0;36mmap_partitions\u001b[0;34m(func, meta, enforce_metadata, transform_divisions, align_dataframes, *args, **kwargs)\u001b[0m\n\u001b[1;32m   6618\u001b[0m         args \u001b[39m=\u001b[39m _maybe_align_partitions(args)\n\u001b[1;32m   6619\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m-> 6620\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   6621\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m. If you don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt want the partitions to be aligned, and are \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   6622\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mcalling `map_partitions` directly, pass `align_dataframes=False`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   6623\u001b[0m         ) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   6625\u001b[0m dfs \u001b[39m=\u001b[39m [df \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m args \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(df, _Frame)]\n\u001b[1;32m   6627\u001b[0m meta \u001b[39m=\u001b[39m _get_meta_map_partitions(args, dfs, func, kwargs, meta, parent_meta)\n",
      "\u001b[0;31mValueError\u001b[0m: Not all divisions are known, can't align partitions. Please use `set_index` to set the index.. If you don't want the partitions to be aligned, and are calling `map_partitions` directly, pass `align_dataframes=False`."
     ]
    }
   ],
   "source": [
    "columns = [\n",
    "    'siren',\n",
    "    'nic',\n",
    "    'siret',\n",
    "    'typeVoieEtablissement',\n",
    "    'libelleVoieEtablissement',\n",
    "    'codePostalEtablissement',\n",
    "    'libelleCommuneEtablissement',\n",
    "    'codeCommuneEtablissement',\n",
    "    'activitePrincipaleEtablissement',\n",
    "    'nomenclatureActivitePrincipaleEtablissement']  # Colonne à utiliser dans la base Siren\n",
    "dtypes = {\n",
    "    'siret': 'string',\n",
    "    'typeVoieEtablissement': 'string',\n",
    "    'libelleVoieEtablissement': 'string',\n",
    "    'codePostalEtablissement': 'string',\n",
    "    'libelleCommuneEtablissement': 'string',\n",
    "    'codeCommuneEtablissement': 'object',\n",
    "}\n",
    "ddcache = dd.read_csv(r\"/home/gaspard/Documents/decp-augmente/data/StockEtablissement_utf8.csv\", sep=',', encoding='utf-8', usecols=columns, dtype=dtypes)\n",
    "ddtest = ddcache.map_partitions( find_missing_siret, dfSIRET, dfcache=pd.DataFrame(data={\"siret\": [\"empty_cache\"]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIn du vestige"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Est ce append ou concat est le plsu rapide ? <br>\n",
    "Append semble être un chouilla plus rapide <br>\n",
    "Même si ça va à l'inverse de ce qui est dit sur internet, mais notre cas d'utilisation est un peu différent. <br>\n",
    "Mais c'est vraiment négligeable ici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcachee = loading_cache(r\"/home/gaspard/Documents/decp-augmente/cache/cache_StockEtablissement_utf8.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n",
      "/tmp/ipykernel_40848/963475162.py:4: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dfcache = dfcache.append(gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.siret.tolist())].copy())\n"
     ]
    }
   ],
   "source": [
    "dfcache = actualiser_cache(dfSIRET, r\"/home/gaspard/Documents/decp-augmente/data/StockEtablissement_utf8.csv\", dfcache=pd.DataFrame())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_on_column_match(dfSIRET: pd.DataFrame, dfcache: pd.DataFrame, column: str):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    -------------\n",
    "    Two dataframes\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    Two series, the second one is the object in column are matching in both dataframes (so the elements in cache), the first one is the one not matching n column.\n",
    "    \"\"\"\n",
    "    boolean_mask = dfSIRET.loc[:, str(column)].isin(dfcache.loc[:, str(column)].tolist())\n",
    "    return dfSIRET.loc[~boolean_mask, str(column)].copy(), dfSIRET.loc[boolean_mask, str(column)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_cache(path_to_cache):\n",
    "    with open(path_to_cache, 'rb') as df_cache:\n",
    "        df = pickle.load(df_cache)\n",
    "    return df\n",
    "\n",
    "def actualiser_cache(dfSiret_to_add, path_to_db, dfcache, chunksize=1000000):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ----------\n",
    "    dfSiret_to_add (Series) avec une unique colonne\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "    \n",
    "\n",
    "    La fonction parcourt la bdd Insee par chunk. Pour chaque chunk on regarde si il y a des correspondances de siret entre dfSiret_to_add et la bdd insee.\n",
    "    Si il y a un match, on ajoute alors les lignes de la bdd insee au dataframe cache. Sinon c'est que les siret sont à la fois valide, mais non présent dans le cache.\n",
    "    On les sépare (on retire ceux trouvé de dfSIRET_to_add) pour pouvoir les mettre dans un second cache.\n",
    "    \"\"\"\n",
    "\n",
    "    for gm_chunk in pd.read_csv(path_to_db, chunksize=chunksize, sep=',', encoding='utf-8', usecols=columns, dtype=dtypes):\n",
    "        # Ajouter à df cache les infos qu'il faut\n",
    "        matching = gm_chunk.loc[gm_chunk.siret.isin(dfSiret_to_add.tolist())].copy() # La copie du dataframe qui match parmis le chunk en cours\n",
    "        dfSiret_to_add = dfSiret_to_add[~dfSiret_to_add.isin(matching.siret.tolist())] \n",
    "        dfcache = dfcache.append(matching)\n",
    "    return dfcache, dfSiret_to_add\n",
    "def write_cache(dfcache, path_to_cache):\n",
    "    with open(path_to_cache, 'wb') as pathcache:\n",
    "        pickle.dump(dfcache, pathcache)\n",
    "        print('cache ecrit')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_luhn_valid(x: int) -> bool:\n",
    "    \"\"\"\n",
    "    Application de la formule de Luhn à un nombre\n",
    "    Permet la verification du numero SIREN et Siret d'un acheteur/etablissement\n",
    "\n",
    "    Retour:\n",
    "        - bool\n",
    "    \"\"\"\n",
    "    try:\n",
    "        luhn_corr = [0, 2, 4, 6, 8, 1, 3, 5, 7, 9]\n",
    "        list_number_in_x = [int(i) for i in list(str(x))]\n",
    "        l2 = [luhn_corr[i] if (index + 1) % 2 == 0 else i for index, i in enumerate(list_number_in_x[::-1])]\n",
    "        if sum(l2) % 10 == 0:\n",
    "            return True\n",
    "        elif str(x)[:9] == \"356000000\":  # SIREN de la Poste\n",
    "            if sum(list_number_in_x) % 5 == 0:\n",
    "                return True\n",
    "        return False\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrichissement_insee_cache(dfSIRET: pd.DataFrame, path_to_data: str, path_to_cache_bdd: str, path_to_cache_not_inbdd: str) -> list:\n",
    "    \"\"\"\n",
    "    Ajout des informations Adresse/Activité des entreprises via la base siren Insee par un système de double cache.\n",
    "    Pour bien comprendre la fonction il y a plusieurs cas possibles concernant un SIRET. Il y a le cas où le siret est valide et match avec la bdd insee (on gère ça avec le premier cache)\n",
    "    Le cas où le siret est invalide ou OOOOOOOO (siret artificiel inscrit en amont dans enrichissement.py) il n'y a aucune chance de trouver ça dans la bdd insee donc.\n",
    "    Le dernier cas où un siret est valide mais pas présent en bdd, pour ceux-ci on créé un second cache.\n",
    "    Dans un cache (dfcache) sont stockés les informations en rpovenance de la bdD insee que l'on gère \n",
    "    Dans le second cache (list_siret_not_found) sont stockés les siret valides que l'on doit gérer mais qui ne sotn pas dans la bdd insee\n",
    "\n",
    "    Arguments\n",
    "    -----------------\n",
    "    dfSIRET \n",
    "    path_to_data chemin vers le fichier csv de la BdD Insee Etablissement_utf8\n",
    "    path_to_cache_bdd chemin vers le cache du fichier csv de la BdD Insee 'Etablissement_utf8'\n",
    "    path_to_cache_not_inbdd:  chemin vers le cache des siret non trouvé dans le fichier csv de la BdD Insee Etablissement_utf8\n",
    "\n",
    "\n",
    "    Returns\n",
    "    --------------\n",
    "        - list:\n",
    "            - list[0]: pd.DataFrame -- données principales\n",
    "            - list[1]: pd.DataFrame -- données où le SIRET n'est pas renseigné\n",
    "\n",
    "    \"\"\"\n",
    "    columns = [\n",
    "        'siren',\n",
    "        'nic',\n",
    "        'siret',\n",
    "        'typeVoieEtablissement',\n",
    "        'libelleVoieEtablissement',\n",
    "        'codePostalEtablissement',\n",
    "        'libelleCommuneEtablissement',\n",
    "        'codeCommuneEtablissement',\n",
    "        'activitePrincipaleEtablissement',\n",
    "        'nomenclatureActivitePrincipaleEtablissement']  # Colonne à utiliser dans la base Siren\n",
    "    dtypes = {\n",
    "        'siret': 'string',\n",
    "        'typeVoieEtablissement': 'string',\n",
    "        'libelleVoieEtablissement': 'string',\n",
    "        'codePostalEtablissement': 'string',\n",
    "        'libelleCommuneEtablissement': 'string',\n",
    "        'codeCommuneEtablissement': 'object',\n",
    "    }\n",
    "    # Traitement pour le cache. Si le siret n'est pas valide ou non renseigné, on va aller chercher dans le cache. Or on veut pas ça, donc on le gère en amont du cache.\n",
    "    # Ceux qui ont un siret non valide on les vire de df SIRET, on les récupèrera plus tard.\n",
    "    mask_siret_nonvalide = (~dfSIRET.siret.apply(is_luhn_valid)) | (dfSIRET.siret == '00000000000000')\n",
    "    dfSIRET_siret_nonvalide = dfSIRET[mask_siret_nonvalide]\n",
    "    dfSIRET = dfSIRET[~mask_siret_nonvalide]\n",
    "    \n",
    "    # Traitons les caches maintenant\n",
    "\n",
    "    #Le second cache des siret valide not found est traité en premier.\n",
    "    cache_siret_not_found_exist = os.path.isfile(path_to_cache_not_inbdd)\n",
    "    if cache_siret_not_found_exist:\n",
    "        list_siret_not_found = loading_cache(path_to_cache_not_inbdd)\n",
    "    else:\n",
    "        list_siret_not_found = []\n",
    "    \n",
    "\n",
    "    mask_siret_valide_notfound = dfSIRET.siret.isin(list_siret_not_found)\n",
    "    dfSIRET_valide_notfound = dfSIRET[mask_siret_valide_notfound]\n",
    "    # On retire les siret valides mais non trouvés lors des précédents passages du df.\n",
    "    dfSIRET = dfSIRET[~mask_siret_valide_notfound]\n",
    "    \n",
    "    cache_exist = os.path.isfile(path_to_cache_bdd)\n",
    "    if cache_exist:\n",
    "        print('loading cache')\n",
    "        dfcache = loading_cache(path_to_cache_bdd)\n",
    "        # regarder les siret dans le cache, ceux pas dans le cache on va passer à travers la bdd pour les trouver. Ceux qui ne sont pas dans la BdD sont sauvés dans un 2e cache.\n",
    "        seriesSIRETnotincache, seriesSIRETincache = split_on_column_match(dfSIRET, dfcache, column=\"siret\")\n",
    "        besoin_actualiser_cache = not(seriesSIRETnotincache.empty)\n",
    "\n",
    "        if besoin_actualiser_cache:\n",
    "            print('actualiser_cache')\n",
    "            # Ceux pas dans le cache, ajouter au cache leur correspondant bddinsee\n",
    "            dfcache, seriessiret_valide_mais_not_found_in_bdd = actualiser_cache(seriesSIRETnotincache, path_to_data, dfcache)\n",
    "            dfcache = dfcache.drop_duplicates(subset=['siret'], keep='first')\n",
    "            #Update cache de la lsite des sirets valides mais non trouvés\n",
    "            list_siret_not_found += seriessiret_valide_mais_not_found_in_bdd.tolist()\n",
    "            \n",
    "            # Actualise les caches\n",
    "            write_cache(dfcache, path_to_cache_bdd)\n",
    "            write_cache(list_siret_not_found, path_to_cache_not_inbdd)\n",
    "            \n",
    "    else:\n",
    "        print('création cache')\n",
    "        #dfSIRET_to_add = split_on_column_match(dfSIRET, pd.DataFrame(data={\"siret\": [\"empty_cache\"]}), column=\"siret\")\n",
    "        # crécupérer le dataframe correspondant au cache\n",
    "        dfcache, seriessiret_valide_mais_not_found_in_bdd = actualiser_cache(dfSIRET.siret, path_to_data, dfcache=pd.DataFrame())\n",
    "        dfcache = dfcache.drop_duplicates(subset=['siret'], keep='first')\n",
    "        # Créer les cache\n",
    "        write_cache(dfcache, path_to_cache_bdd)\n",
    "        write_cache(seriessiret_valide_mais_not_found_in_bdd.tolist(), path_to_cache_not_inbdd)\n",
    "    enrichissement_insee_siret = pd.merge(dfSIRET, dfcache, how='outer', on=['siret'], copy=False)\n",
    "    enrichissement_insee_siret.rename(columns={\"siren_x\": \"siren\"}, inplace=True)\n",
    "    enrichissement_insee_siret.drop(columns=[\"siren_y\"], axis=1, inplace=True)\n",
    "    nanSiret = pd.concat([enrichissement_insee_siret[enrichissement_insee_siret.activitePrincipaleEtablissement.isnull()], dfSIRET_siret_nonvalide, dfSIRET_valide_notfound])\n",
    "    enrichissement_insee_siret = enrichissement_insee_siret[\n",
    "        enrichissement_insee_siret.activitePrincipaleEtablissement.notnull()]\n",
    "    nanSiret = nanSiret.loc[:, [\"siret\", \"siren\", \"denominationSociale\"]]\n",
    "\n",
    "    # Concaténation des deux resultats\n",
    "    enrichissementInsee = enrichissement_insee_siret\n",
    "    nanSiret = nanSiret.iloc[:, :3]\n",
    "    nanSiret.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return [enrichissementInsee, nanSiret]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_enrichissement_insee(dfSIRET: pd.DataFrame, path_to_data: str) -> list:\n",
    "    \"\"\"\n",
    "    Ajout des informations Adresse/Activité des entreprises via la base siren Insee\n",
    "\n",
    "    Retour:\n",
    "        - list:\n",
    "            - list[0]: pd.DataFrame -- données principales\n",
    "            - list[1]: pd.DataFrame -- données ou le SIRET n'est pas renseigné\n",
    "    \"\"\"\n",
    "    # dans StockEtablissement_utf8, il y a principalement : siren, siret, nom établissement, adresse, activité principale\n",
    "    path = path_to_data\n",
    "    columns = [\n",
    "        'siren',\n",
    "        'nic',\n",
    "        'siret',\n",
    "        'typeVoieEtablissement',\n",
    "        'libelleVoieEtablissement',\n",
    "        'codePostalEtablissement',\n",
    "        'libelleCommuneEtablissement',\n",
    "        'codeCommuneEtablissement',\n",
    "        'activitePrincipaleEtablissement',\n",
    "        'nomenclatureActivitePrincipaleEtablissement']  # Colonne à utiliser dans la base Siren\n",
    "    dtypes = {\n",
    "        'siret': 'string',\n",
    "        'typeVoieEtablissement': 'string',\n",
    "        'libelleVoieEtablissement': 'string',\n",
    "        'codePostalEtablissement': 'string',\n",
    "        'libelleCommuneEtablissement': 'string',\n",
    "    }\n",
    "\n",
    "    result = pd.DataFrame(columns=columns)\n",
    "    chunksize = 1000000\n",
    "    for gm_chunk in pd.read_csv(path, chunksize=chunksize, sep=',', encoding='utf-8', usecols=columns, dtype=dtypes):\n",
    "        resultTemp = pd.merge(dfSIRET['siret'], gm_chunk, on=['siret'], copy=False)\n",
    "        result = pd.concat([result, resultTemp], axis=0, copy=False)\n",
    "        del resultTemp\n",
    "    result = result.drop_duplicates(subset=['siret'], keep='first')\n",
    "\n",
    "    enrichissement_insee_siret = pd.merge(dfSIRET, result, how='outer', on=['siret'], copy=False)\n",
    "    enrichissement_insee_siret.rename(columns={\"siren_x\": \"siren\"}, inplace=True)\n",
    "    enrichissement_insee_siret.drop(columns=[\"siren_y\"], axis=1, inplace=True)\n",
    "    nanSiret = enrichissement_insee_siret[enrichissement_insee_siret.activitePrincipaleEtablissement.isnull()]\n",
    "    print('Premier nanSiret \\n', nanSiret)\n",
    "    enrichissement_insee_siret = enrichissement_insee_siret[\n",
    "        enrichissement_insee_siret.activitePrincipaleEtablissement.notnull()]\n",
    "    nanSiret = nanSiret.loc[:, [\"siret\", \"siren\", \"denominationSociale\"]]\n",
    "\n",
    "    # Concaténation des deux resultats\n",
    "    enrichissementInsee = enrichissement_insee_siret\n",
    "    print(enrichissement_insee_siret)\n",
    "    temp_df = pd.merge(nanSiret, result, indicator=True, how=\"outer\", on='siren', copy=False)\n",
    "    del result\n",
    "    nanSiret = temp_df[temp_df['activitePrincipaleEtablissement'].isnull()]\n",
    "    nanSiret = nanSiret.iloc[:, :3]\n",
    "    nanSiret.reset_index(inplace=True, drop=True).drop_duplicates(subset=['siret'], keep='first')\n",
    "\n",
    "    return [enrichissementInsee, nanSiret]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40848/563557681.py:33: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for gm_chunk in pd.read_csv(path, chunksize=chunksize, sep=',', encoding='utf-8', usecols=columns, dtype=dtypes):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [414], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unpack_no_cache_l \u001b[39m=\u001b[39m get_enrichissement_insee(dfSIRET,\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/home/gaspard/Documents/decp-augmente/data/StockEtablissement_utf8.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [413], line 33\u001b[0m, in \u001b[0;36mget_enrichissement_insee\u001b[0;34m(dfSIRET, path_to_data)\u001b[0m\n\u001b[1;32m     31\u001b[0m result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39mcolumns)\n\u001b[1;32m     32\u001b[0m chunksize \u001b[39m=\u001b[39m \u001b[39m1000000\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfor\u001b[39;00m gm_chunk \u001b[39min\u001b[39;00m pd\u001b[39m.\u001b[39mread_csv(path, chunksize\u001b[39m=\u001b[39mchunksize, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m, usecols\u001b[39m=\u001b[39mcolumns, dtype\u001b[39m=\u001b[39mdtypes):\n\u001b[1;32m     34\u001b[0m     resultTemp \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mmerge(dfSIRET[\u001b[39m'\u001b[39m\u001b[39msiret\u001b[39m\u001b[39m'\u001b[39m], gm_chunk, on\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39msiret\u001b[39m\u001b[39m'\u001b[39m], copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m     result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([result, resultTemp], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1698\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1696\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m   1697\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1698\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_chunk()\n\u001b[1;32m   1699\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m   1700\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1810\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow)\n\u001b[0;32m-> 1810\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nrows\u001b[39m=\u001b[39;49msize)\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:820\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:890\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1037\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1083\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/_libs/parsers.pyx:1158\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Documents/env/augmente/lib/python3.8/site-packages/pandas/core/dtypes/common.py:1433\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1424\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1425\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[1;32m   1426\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1427\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1428\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1429\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1430\u001b[0m     )\n\u001b[0;32m-> 1433\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m   1434\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1436\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1476\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[1;32m   1477\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "unpack_no_cache_l = get_enrichissement_insee(dfSIRET,r'/home/gaspard/Documents/decp-augmente/data/StockEtablissement_utf8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cache\n",
      "                siret      siren denominationSociale\n",
      "15144  95651314700001  956513147             DECITRE\n",
      "                siret      siren denominationSociale  nic  \\\n",
      "15144  95651314700001  956513147             DECITRE  NaN   \n",
      "\n",
      "      typeVoieEtablissement libelleVoieEtablissement codePostalEtablissement  \\\n",
      "15144                  <NA>                     <NA>                    <NA>   \n",
      "\n",
      "      libelleCommuneEtablissement codeCommuneEtablissement  \\\n",
      "15144                        <NA>                      NaN   \n",
      "\n",
      "      activitePrincipaleEtablissement  \\\n",
      "15144                             NaN   \n",
      "\n",
      "      nomenclatureActivitePrincipaleEtablissement  \n",
      "15144                                         NaN  \n"
     ]
    }
   ],
   "source": [
    "unpack_l = enrichissement_insee_cache(dfSIRET, r'/home/gaspard/Documents/decp-augmente/data/StockEtablissement_utf8.csv', r\"/home/gaspard/Documents/decp-augmente/cache/cache_StockEtablissement_utf8.csv\", r\"/home/gaspard/Documents/decp-augmente/cache/cache_NOTIN_StockEtablissement_utf8.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opti enrichissement_acheteur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('augmente')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5d3eeb2620ffd2d6e4e90ad12ed2892e7d02b9b73a71895a648c9794ab782bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
